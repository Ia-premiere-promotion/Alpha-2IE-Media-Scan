"""
Routes API pour le pipeline de scraping automatique
Permet de lancer, suivre et r√©cup√©rer les r√©sultats des scraping
"""

from flask import Blueprint, jsonify, request
from datetime import datetime
import threading
import os
import sys
import json
from pathlib import Path

# Ajouter le path du pipeline
sys.path.insert(0, str(Path(__file__).parent.parent / 'pipeline'))
sys.path.insert(0, str(Path(__file__).parent.parent))

from pipeline.orchestrator import PipelineOrchestrator
from supabase_client import get_supabase_client

pipeline_bp = Blueprint('pipeline', __name__, url_prefix='/api/pipeline')

# Fichier pour persister l'√©tat du pipeline
STATE_FILE = Path(__file__).parent.parent / 'pipeline_state.json'

def load_pipeline_state():
    """Charge l'√©tat du pipeline depuis le fichier"""
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    return {
        'is_running': False,
        'last_run': None,
        'last_result': None,
        'current_progress': None,
        'notifications': []
    }

def save_pipeline_state(state):
    """Sauvegarde l'√©tat du pipeline dans le fichier"""
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ùå Erreur sauvegarde √©tat: {e}")

# Charger l'√©tat initial
pipeline_state = load_pipeline_state()


def run_pipeline_async(max_articles=20):
    """Ex√©cute le pipeline en arri√®re-plan"""
    global pipeline_state
    
    try:
        pipeline_state['is_running'] = True
        pipeline_state['current_progress'] = {
            'status': 'starting',
            'message': 'üöÄ D√©marrage du scraping...',
            'timestamp': datetime.now().isoformat()
        }
        save_pipeline_state(pipeline_state)
        
        # Ajouter notification de d√©marrage
        add_notification({
            'type': 'info',
            'title': 'Scraping d√©marr√©',
            'message': 'Le scraping automatique a commenc√©',
            'timestamp': datetime.now().isoformat()
        })
        
        # Cr√©er et ex√©cuter l'orchestrateur
        orchestrator = PipelineOrchestrator(include_facebook=False)
        
        pipeline_state['current_progress'] = {
            'status': 'scraping',
            'message': 'üì∞ Scraping des m√©dias en cours...',
            'timestamp': datetime.now().isoformat()
        }
        save_pipeline_state(pipeline_state)
        
        stats = orchestrator.run_full_pipeline(
            max_articles_per_section=max_articles,
            facebook_max_posts=0
        )
        
        # Pipeline termin√©
        pipeline_state['is_running'] = False
        pipeline_state['last_run'] = datetime.now().isoformat()
        pipeline_state['last_result'] = {
            'success': True,
            'stats': stats,
            'timestamp': datetime.now().isoformat(),
            'duration': stats.get('duration', 0)
        }
        pipeline_state['current_progress'] = None
        save_pipeline_state(pipeline_state)
        
        # Ajouter notification de succ√®s
        add_notification({
            'type': 'success',
            'title': 'Scraping termin√©',
            'message': f"{stats['total_inserted']} nouveaux articles ins√©r√©s",
            'stats': stats,
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"‚úÖ Pipeline termin√©: {stats}")
        
    except Exception as e:
        pipeline_state['is_running'] = False
        pipeline_state['last_run'] = datetime.now().isoformat()
        pipeline_state['last_result'] = {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        pipeline_state['current_progress'] = None
        save_pipeline_state(pipeline_state)
        
        # Ajouter notification d'erreur
        add_notification({
            'type': 'error',
            'title': 'Erreur de scraping',
            'message': f'Erreur: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })
        
        print(f"‚ùå Erreur pipeline: {e}")
        import traceback
        traceback.print_exc()


def add_notification(notification):
    """Ajoute une notification √† la liste (max 20)"""
    global pipeline_state
    pipeline_state['notifications'].insert(0, notification)
    # Garder seulement les 20 derni√®res
    pipeline_state['notifications'] = pipeline_state['notifications'][:20]
    save_pipeline_state(pipeline_state)
    print(f"üì¢ Notification ajout√©e: {notification['title']}")


@pipeline_bp.route('/run', methods=['POST'])
def run_pipeline():
    """Lance le pipeline de scraping en arri√®re-plan"""
    global pipeline_state
    
    if pipeline_state['is_running']:
        return jsonify({
            'success': False,
            'message': 'Un scraping est d√©j√† en cours'
        }), 400
    
    # Param√®tres optionnels - g√©rer les diff√©rents formats de requ√™te
    try:
        max_articles = request.json.get('max_articles', 20) if request.json and isinstance(request.json, dict) else 20
    except:
        max_articles = 20
    
    # Lancer le pipeline dans un thread s√©par√©
    thread = threading.Thread(target=run_pipeline_async, args=(max_articles,))
    thread.daemon = True
    thread.start()
    
    return jsonify({
        'success': True,
        'message': 'Pipeline d√©marr√© en arri√®re-plan',
        'timestamp': datetime.now().isoformat()
    })


@pipeline_bp.route('/status', methods=['GET'])
def get_status():
    """Retourne l'√©tat actuel du pipeline"""
    # Recharger l'√©tat depuis le fichier pour avoir les derni√®res donn√©es
    global pipeline_state
    pipeline_state = load_pipeline_state()
    
    return jsonify({
        'is_running': pipeline_state['is_running'],
        'last_run': pipeline_state['last_run'],
        'last_result': pipeline_state['last_result'],
        'current_progress': pipeline_state['current_progress']
    })


@pipeline_bp.route('/notifications', methods=['GET'])
def get_notifications():
    """Retourne les notifications de scraping"""
    # Recharger l'√©tat depuis le fichier
    global pipeline_state
    pipeline_state = load_pipeline_state()
    
    # Filtrer par type si sp√©cifi√©
    notification_type = request.args.get('type')
    
    notifications = pipeline_state['notifications']
    if notification_type:
        notifications = [n for n in notifications if n['type'] == notification_type]
    
    print(f"üìã Retour {len(notifications)} notifications")
    
    return jsonify({
        'notifications': notifications,
        'count': len(notifications),
        'unread_count': len([n for n in pipeline_state['notifications'] if not n.get('read', False)])
    })


@pipeline_bp.route('/notifications/mark-read', methods=['POST'])
def mark_notifications_read():
    """Marque toutes les notifications comme lues"""
    global pipeline_state
    pipeline_state = load_pipeline_state()
    
    for notification in pipeline_state['notifications']:
        notification['read'] = True
    
    save_pipeline_state(pipeline_state)
    
    return jsonify({
        'success': True,
        'message': 'Notifications marqu√©es comme lues'
    })


@pipeline_bp.route('/notifications/clear', methods=['POST'])
def clear_notifications():
    """Efface toutes les notifications"""
    global pipeline_state
    pipeline_state = load_pipeline_state()
    pipeline_state['notifications'] = []
    save_pipeline_state(pipeline_state)
    
    return jsonify({
        'success': True,
        'message': 'Notifications effac√©es'
    })


@pipeline_bp.route('/history', methods=['GET'])
def get_history():
    """Retourne l'historique des scrappings depuis la base de donn√©es"""
    try:
        supabase = get_supabase_client()
        
        # Param√®tres de pagination
        limit = request.args.get('limit', 50, type=int)
        offset = request.args.get('offset', 0, type=int)
        
        # R√©cup√©rer l'historique des scrappings
        result = supabase.table('scraping_logs')\
            .select('*')\
            .order('started_at', desc=True)\
            .range(offset, offset + limit - 1)\
            .execute()
        
        logs = result.data
        
        # Enrichir avec les d√©tails par m√©dia
        for log in logs:
            # R√©cup√©rer les d√©tails par m√©dia pour ce scraping
            media_details = supabase.table('scraping_media_details')\
                .select('*, medias(name)')\
                .eq('scraping_log_id', log['id'])\
                .execute()
            
            log['media_details'] = media_details.data
        
        return jsonify({
            'success': True,
            'logs': logs,
            'count': len(logs)
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@pipeline_bp.route('/history/<int:log_id>', methods=['GET'])
def get_history_detail(log_id):
    """Retourne les d√©tails d'un scraping sp√©cifique"""
    try:
        supabase = get_supabase_client()
        
        # R√©cup√©rer le log
        log_result = supabase.table('scraping_logs')\
            .select('*')\
            .eq('id', log_id)\
            .single()\
            .execute()
        
        if not log_result.data:
            return jsonify({
                'success': False,
                'error': 'Log non trouv√©'
            }), 404
        
        log = log_result.data
        
        # R√©cup√©rer les d√©tails par m√©dia
        media_details = supabase.table('scraping_media_details')\
            .select('*, medias(name, logo)')\
            .eq('scraping_log_id', log_id)\
            .execute()
        
        log['media_details'] = media_details.data
        
        return jsonify({
            'success': True,
            'log': log
        })
    
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

