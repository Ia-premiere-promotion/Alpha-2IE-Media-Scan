#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Orchestrateur principal du pipeline de scraping
Coordonne: Scraping (Web + Facebook) ‚Üí ML Prediction ‚Üí Cleaning ‚Üí Database Insertion ‚Üí CSV Export
"""

import sys
import os
import csv
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv

# Charger variables d'environnement
load_dotenv()

# Ajouter le path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

# Importations des modules du pipeline
from scrapers.web.lefaso_scraper import LeFasoScraper
from scrapers.web.sidwaya_scraper import SidwayaScraper
from scrapers.web.fasopresse_scraper import FasoPresseScraper
from scrapers.web.observateur_scraper import ObservateurScraper
from scrapers.web.burkina24_scraper import Burkina24Scraper
# Facebook scraper retir√© - on utilise les JSON d√©j√† scrap√©s
from ml.predictor import CategoryPredictor
from utils.cleaner import DataCleaner
from utils.db_writer import DatabaseWriter
from utils.date_manager import DateManager
from supabase_client import get_supabase_client


class PipelineOrchestrator:
    """Orchestre l'ex√©cution compl√®te du pipeline de scraping"""
    
    def __init__(self, include_facebook=False):
        print(f"\n{'='*70}")
        print(f"üöÄ PIPELINE DE SCRAPING AUTOMATIQUE - MONITORING M√âDIATIQUE")
        print(f"{'='*70}")
        print(f"D√©marr√©: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # Initialiser le client Supabase
        self.supabase = get_supabase_client()
        
        # Initialiser le gestionnaire de dates
        self.date_manager = DateManager()
        
        # Initialiser les scrapers WEB
        self.scrapers = [
            LeFasoScraper(),
            SidwayaScraper(),
            FasoPresseScraper(),
            ObservateurScraper(),
            Burkina24Scraper()
        ]
        
        # Facebook scraping d√©sactiv√© - on utilise les JSON d√©j√† g√©n√©r√©s
        self.include_facebook = False
        self.facebook_scraper = None
        
        # Configurer la date de derni√®re publication pour chaque scraper web
        for scraper in self.scrapers:
            last_date = self.date_manager.get_last_date(scraper.media_name)
            scraper.set_last_publication_date(last_date)
            print(f"üìÖ {scraper.media_name}: Derni√®re publication = {last_date.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # Initialiser les modules
        self.predictor = CategoryPredictor()
        self.cleaner = DataCleaner()
        self.db_writer = DatabaseWriter()
        
        # Statistiques
        self.stats = {
            'total_scraped': 0,
            'total_cleaned': 0,
            'total_inserted': 0,
            'total_skipped': 0,
            'total_errors': 0
        }
        
        # D√©tails par m√©dia (pour scraping_media_details)
        self.media_stats = {}
        
        # ID du log en cours (pour enregistrement en BD)
        self.scraping_log_id = None
        
        # Liste des fichiers CSV temporaires cr√©√©s (pour suppression auto)
        self.temp_csv_files = []
    
    def run_scraping(self, max_articles_per_section=20, facebook_max_posts=50):
        """
        √âtape 1: Scrape tous les m√©dias (Web + Facebook)
        
        Args:
            max_articles_per_section: Nombre max d'articles par section (web)
            facebook_max_posts: Nombre max de posts Facebook
        
        Returns:
            Liste d'articles bruts
        """
        print(f"\n{'='*70}")
        print(f"üì∞ √âTAPE 1: SCRAPING DES M√âDIAS")
        print(f"{'='*70}\n")
        
        all_articles = []
        
        # Scraper les sites web
        for scraper in self.scrapers:
            try:
                articles = scraper.scrape_all_sections(max_articles_per_section)
                all_articles.extend(articles)
                
                # Enregistrer les stats par m√©dia
                media_name = scraper.media_name
                if media_name not in self.media_stats:
                    self.media_stats[media_name] = {
                        'scraped': 0,
                        'inserted': 0,
                        'skipped': 0,
                        'last_article_date': None
                    }
                
                self.media_stats[media_name]['scraped'] = len(articles)
                
                # Trouver la date du dernier article
                if articles:
                    dates = [a.get('date') for a in articles if a.get('date')]
                    if dates:
                        self.media_stats[media_name]['last_article_date'] = max(dates)
                
                print(f"‚úÖ {scraper.media_name}: {len(articles)} articles scrap√©s")
            except Exception as e:
                print(f"‚ùå Erreur scraping {scraper.media_name}: {e}")
        
        # Facebook scraping d√©sactiv√© - utilisera les JSON via facebook_orchestrator.py s√©par√©
        
        self.stats['total_scraped'] = len(all_articles)
        print(f"\n‚úÖ Total scrap√©: {len(all_articles)} articles de {len(self.scrapers)} sources")
        
        return all_articles
    
    def run_prediction(self, articles):
        """
        √âtape 2: Pr√©dire les cat√©gories avec ML
        
        Args:
            articles: Liste d'articles
        
        Returns:
            Liste d'articles avec cat√©gories pr√©dites
        """
        print(f"\n{'='*70}")
        print(f"ü§ñ √âTAPE 2: PR√âDICTION DES CAT√âGORIES (ML)")
        print(f"{'='*70}")
        
        articles = self.predictor.predict_batch(articles)
        
        return articles
    
    def run_cleaning(self, articles):
        """
        √âtape 3: Nettoyer et valider les donn√©es
        
        Args:
            articles: Liste d'articles
        
        Returns:
            Liste d'articles nettoy√©s et valid√©s
        """
        print(f"\n{'='*70}")
        print(f"üßπ √âTAPE 3: NETTOYAGE ET VALIDATION")
        print(f"{'='*70}")
        
        # Nettoyer
        articles = self.cleaner.clean_batch(articles)
        
        # Supprimer les doublons
        articles = self.cleaner.deduplicate(articles)
        
        self.stats['total_cleaned'] = len(articles)
        
        return articles
    
    def run_insertion(self, articles):
        """
        √âtape 4: Ins√©rer dans la base de donn√©es
        
        Args:
            articles: Liste d'articles nettoy√©s
        
        Returns:
            Statistiques d'insertion
        """
        print(f"\n{'='*70}")
        print(f"üíæ √âTAPE 4: INSERTION DANS SUPABASE")
        print(f"{'='*70}")
        
        stats = self.db_writer.insert_batch(articles)
        
        self.stats['total_inserted'] = stats['inserted']
        self.stats['total_skipped'] = stats['skipped']
        self.stats['total_errors'] = stats['errors']
        
        return stats
    
    def export_to_csv(self, articles, filename=None):
        """
        Exporte les articles vers un fichier CSV avec validation stricte
        
        Args:
            articles: Liste d'articles √† exporter
            filename: Nom du fichier CSV (optionnel)
        
        Returns:
            str: Chemin du fichier CSV cr√©√©
        """
        print(f"\n{'='*70}")
        print(f"üìÑ EXPORT CSV DES ARTICLES")
        print(f"{'='*70}")
        
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'articles_export_{timestamp}.csv'
        
        # Chemin complet
        csv_path = Path(__file__).parent / filename
        
        # Champs CSV (selon sch√©ma BD exactement)
        fieldnames = [
            'id', 'media', 'titre', 'contenu', 'url', 'date', 
            'categorie', 'type_source', 'plateforme',
            'likes', 'commentaires', 'partages'
        ]
        
        exported = 0
        skipped = 0
        
        try:
            with open(csv_path, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for article in articles:
                    try:
                        # Validation stricte avant export
                        is_valid, error_msg = self.db_writer.validate_article_for_db(article)
                        if not is_valid:
                            print(f"‚ö†Ô∏è Ligne ignor√©e (invalide): {error_msg}")
                            skipped += 1
                            continue
                        
                        # Pr√©parer la ligne CSV (selon sch√©ma BD)
                        row = {
                            'id': article.get('id', ''),
                            'media': article.get('media', ''),
                            'titre': article.get('titre', '').replace('\n', ' ').replace('\r', ''),
                            'contenu': article.get('contenu', '').replace('\n', ' ').replace('\r', ''),
                            'url': article.get('url', ''),
                            'date': article['date'].isoformat() if hasattr(article['date'], 'isoformat') else str(article['date']),
                            'categorie': article.get('categorie', ''),
                            'type_source': article.get('type_source', 'article'),
                            'plateforme': article.get('plateforme', 'web'),
                            'likes': article.get('likes', 0),
                            'commentaires': article.get('commentaires', 0),
                            'partages': article.get('partages', 0)
                        }
                        
                        writer.writerow(row)
                        exported += 1
                        
                    except Exception as e:
                        print(f"‚ùå Erreur export ligne: {e} - Article ignor√©")
                        skipped += 1
                        continue
            
            print(f"‚úÖ Export CSV termin√©:")
            print(f"   - Export√©s: {exported}")
            if skipped > 0:
                print(f"   - Ignor√©s: {skipped}")
            print(f"   - Fichier: {csv_path}")
            
            # Ajouter √† la liste des fichiers temporaires pour suppression ult√©rieure
            self.temp_csv_files.append(str(csv_path))
            
            return str(csv_path)
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation du fichier CSV: {e}")
            return None
    
    def run_full_pipeline(self, max_articles_per_section=20, facebook_max_posts=50):
        """
        Ex√©cute le pipeline complet
        
        Args:
            max_articles_per_section: Nombre max d'articles par section (d√©faut: 20)
            facebook_max_posts: Nombre max de posts Facebook (d√©faut: 50)
        
        Returns:
            dict: Statistiques compl√®tes
        """
        start_time = datetime.now()
        
        # Cr√©er l'entr√©e dans scraping_logs
        try:
            log_entry = self.supabase.table('scraping_logs').insert({
                'started_at': start_time.isoformat(),
                'status': 'running',
                'total_scraped': 0,
                'total_inserted': 0,
                'total_skipped': 0,
                'total_errors': 0
            }).execute()
            
            self.scraping_log_id = log_entry.data[0]['id']
            print(f"üìù Log de scraping cr√©√©: ID={self.scraping_log_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur cr√©ation log scraping: {e}")
            self.scraping_log_id = None
        
        try:
            # 1. Scraping
            articles = self.run_scraping(max_articles_per_section, facebook_max_posts)
            
            if not articles:
                print("\n‚ö†Ô∏è Aucun article scrap√©. Arr√™t du pipeline.")
                self._update_scraping_log(start_time, 'completed')
                return self.stats
            
            # 2. Pr√©diction ML
            articles = self.run_prediction(articles)
            
            # 3. Nettoyage
            articles = self.run_cleaning(articles)
            
            if not articles:
                print("\n‚ö†Ô∏è Aucun article valide apr√®s nettoyage. Arr√™t du pipeline.")
                self._update_scraping_log(start_time, 'completed')
                return self.stats
            
            # 4. Export CSV BRUT (avant validation BD)
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            csv_brut_path = self.export_to_csv(articles, f'articles_brut_{timestamp}.csv')
            if csv_brut_path:
                print(f"‚úÖ CSV brut export√©: {csv_brut_path}")
            
            # 5. Validation stricte selon sch√©ma BD
            print(f"\n{'='*70}")
            print(f"üîß VALIDATION STRICTE SELON SCH√âMA BD")
            print(f"{'='*70}\n")
            
            validated_articles = []
            rejected_articles = []
            
            print("üîç Validation et nettoyage en cours...")
            for i, article in enumerate(articles, 1):
                # Nettoyer l'article
                cleaned_article = self.db_writer.clean_article_for_db(article)
                
                # Valider strictement
                is_valid, error_msg = self.db_writer.validate_article_for_db(cleaned_article)
                
                if is_valid:
                    validated_articles.append(cleaned_article)
                else:
                    rejected_articles.append({
                        'article': cleaned_article,
                        'reason': error_msg
                    })
                    print(f"   ‚äò [{i}/{len(articles)}] Rejet√©: {error_msg}")
            
            print(f"\n‚úÖ Validation termin√©e:")
            print(f"   - Articles valides: {len(validated_articles)}")
            print(f"   - Articles rejet√©s: {len(rejected_articles)}")
            
            # 6. Rapport d√©taill√© des rejets
            if rejected_articles:
                print(f"\n{'='*70}")
                print(f"üìã RAPPORT DES ARTICLES REJET√âS")
                print(f"{'='*70}\n")
                
                # Grouper les rejets par raison
                reject_reasons = {}
                for reject in rejected_articles:
                    reason = reject['reason']
                    if reason not in reject_reasons:
                        reject_reasons[reason] = []
                    reject_reasons[reason].append(reject['article'].get('titre', 'Sans titre')[:50])
                
                for reason, titles in sorted(reject_reasons.items(), key=lambda x: len(x[1]), reverse=True):
                    print(f"‚ùå {reason} ({len(titles)} articles)")
                    for title in titles[:3]:  # Montrer max 3 exemples
                        print(f"   - {title}...")
                    if len(titles) > 3:
                        print(f"   ... et {len(titles)-3} autres")
                    print()
            
            # 7. Export CSV VALID√â (seulement les articles 100% propres)
            if validated_articles:
                csv_valide_path = self.export_to_csv(validated_articles, f'articles_valides_{timestamp}.csv')
                if csv_valide_path:
                    print(f"\n‚úÖ CSV valid√© export√©: {csv_valide_path}")
                    self.stats['csv_exported'] = csv_valide_path
            else:
                print("\n‚ö†Ô∏è Aucun article valide √† exporter !")
            
            # 8. Insertion DB (seulement les articles valid√©s)
            if validated_articles:
                insertion_stats = self.run_insertion(validated_articles)
                
                # Mettre √† jour les stats par m√©dia avec les r√©sultats d'insertion
                if 'by_media' in insertion_stats:
                    for media_name, media_insertion in insertion_stats['by_media'].items():
                        if media_name in self.media_stats:
                            self.media_stats[media_name]['inserted'] = media_insertion['inserted']
                            self.media_stats[media_name]['skipped'] = media_insertion['skipped']
                
                # Enregistrer les d√©tails par m√©dia dans la BD
                self._save_media_details()
            else:
                print("\n‚ö†Ô∏è Aucun article valide pour insertion. Arr√™t.")
            
        except Exception as e:
            print(f"\n‚ùå ERREUR CRITIQUE DANS LE PIPELINE: {e}")
            import traceback
            traceback.print_exc()
            
            # Mettre √† jour le log avec l'erreur
            self._update_scraping_log(start_time, 'failed', error_message=str(e))
        
        finally:
            # Mettre √† jour le log avec les stats finales
            if self.scraping_log_id:
                self._update_scraping_log(start_time, 'completed')
            
            # Supprimer les fichiers CSV temporaires
            self._cleanup_temp_csv_files()
            
            # Afficher le r√©sum√©
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            print(f"\n{'='*70}")
            print(f"üìä R√âSUM√â DU PIPELINE")
            print(f"{'='*70}")
            print(f"Dur√©e totale: {duration:.1f} secondes")
            print(f"\nStatistiques:")
            print(f"  üì∞ Articles scrap√©s: {self.stats['total_scraped']}")
            print(f"  ‚úÖ Articles nettoy√©s: {self.stats['total_cleaned']}")
            print(f"  üíæ Articles ins√©r√©s: {self.stats['total_inserted']}")
            print(f"  ‚è≠Ô∏è  Articles ignor√©s (doublons): {self.stats['total_skipped']}")
            if self.stats['total_errors'] > 0:
                print(f"  ‚ùå Erreurs: {self.stats['total_errors']}")
            if 'csv_exported' in self.stats:
                print(f"  üìÑ CSV export√©: {self.stats['csv_exported']}")
            
            success_rate = (self.stats['total_inserted'] / self.stats['total_scraped'] * 100) if self.stats['total_scraped'] > 0 else 0
            print(f"\n  üéØ Taux de r√©ussite: {success_rate:.1f}%")
            
            print(f"\n{'='*70}")
            print(f"üéâ PIPELINE TERMIN√â")
            print(f"{'='*70}\n")
        
        return self.stats
    
    def _update_scraping_log(self, start_time, status, error_message=None):
        """
        Met √† jour l'entr√©e dans scraping_logs
        
        Args:
            start_time: Datetime du d√©but du scraping
            status: 'running', 'completed', 'failed'
            error_message: Message d'erreur si √©chec
        """
        if not self.scraping_log_id:
            return
        
        try:
            end_time = datetime.now()
            duration = int((end_time - start_time).total_seconds())
            
            update_data = {
                'completed_at': end_time.isoformat(),
                'duration_seconds': duration,
                'status': status,
                'total_scraped': self.stats['total_scraped'],
                'total_inserted': self.stats['total_inserted'],
                'total_skipped': self.stats['total_skipped'],
                'total_errors': self.stats['total_errors']
            }
            
            if error_message:
                update_data['error_message'] = error_message
            
            self.supabase.table('scraping_logs')\
                .update(update_data)\
                .eq('id', self.scraping_log_id)\
                .execute()
            
            print(f"‚úÖ Log de scraping mis √† jour: ID={self.scraping_log_id}, status={status}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur mise √† jour log scraping: {e}")
    
    def _save_media_details(self):
        """
        Enregistre les d√©tails par m√©dia dans scraping_media_details
        """
        if not self.scraping_log_id:
            return
        
        try:
            # R√©cup√©rer les IDs des m√©dias
            medias_result = self.supabase.table('medias').select('id, name').execute()
            media_name_to_id = {m['name']: m['id'] for m in medias_result.data}
            
            # Cr√©er les entr√©es pour chaque m√©dia
            for media_name, stats in self.media_stats.items():
                # Trouver l'ID du m√©dia
                media_id = media_name_to_id.get(media_name)
                if not media_id:
                    print(f"‚ö†Ô∏è M√©dia non trouv√© en BD: {media_name}")
                    continue
                
                # Pr√©parer les donn√©es
                detail_data = {
                    'scraping_log_id': self.scraping_log_id,
                    'media_id': media_id,
                    'articles_scraped': stats.get('scraped', 0),
                    'articles_inserted': stats.get('inserted', 0),
                    'articles_skipped': stats.get('skipped', 0)
                }
                
                # Ajouter la date du dernier article si disponible
                if stats.get('last_article_date'):
                    last_date = stats['last_article_date']
                    if hasattr(last_date, 'isoformat'):
                        detail_data['last_article_date'] = last_date.isoformat()
                    else:
                        detail_data['last_article_date'] = str(last_date)
                
                # Ins√©rer dans la table
                self.supabase.table('scraping_media_details').insert(detail_data).execute()
            
            print(f"‚úÖ D√©tails par m√©dia enregistr√©s ({len(self.media_stats)} m√©dias)")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur enregistrement d√©tails par m√©dia: {e}")
            import traceback
            traceback.print_exc()
    
    def _cleanup_temp_csv_files(self):
        """
        Supprime automatiquement les fichiers CSV temporaires apr√®s utilisation
        """
        if not self.temp_csv_files:
            return
        
        print(f"\n{'='*70}")
        print(f"üßπ NETTOYAGE DES FICHIERS TEMPORAIRES")
        print(f"{'='*70}")
        
        deleted_count = 0
        failed_count = 0
        
        for csv_file in self.temp_csv_files:
            try:
                csv_path = Path(csv_file)
                if csv_path.exists():
                    csv_path.unlink()  # Supprime le fichier
                    print(f"‚úÖ Supprim√©: {csv_path.name}")
                    deleted_count += 1
                else:
                    print(f"‚ö†Ô∏è Fichier introuvable: {csv_path.name}")
            except Exception as e:
                print(f"‚ùå Erreur suppression {csv_path.name}: {e}")
                failed_count += 1
        
        print(f"\nüìä R√©sum√© nettoyage:")
        print(f"   - Fichiers supprim√©s: {deleted_count}")
        if failed_count > 0:
            print(f"   - √âchecs: {failed_count}")
        
        # Vider la liste
        self.temp_csv_files = []


def main():
    """Point d'entr√©e principal"""
    # Orchestrateur Web uniquement - Facebook a son propre syst√®me
    orchestrator = PipelineOrchestrator()
    
    # Ex√©cuter le pipeline Web
    stats = orchestrator.run_full_pipeline(
        max_articles_per_section=20
    )
    
    return stats


if __name__ == "__main__":
    main()