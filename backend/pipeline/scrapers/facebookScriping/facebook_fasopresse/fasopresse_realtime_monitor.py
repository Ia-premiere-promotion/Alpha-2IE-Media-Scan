"""
Script de monitoring temps r√©el de la page Facebook Fasopresse
D√©tecte le dernier post et met √† jour les m√©triques dans un JSON (pas de doublons)
"""

import json
import time
import os
from datetime import datetime
from dotenv import load_dotenv
from facebook_playwright_scraper import FacebookPlaywrightScraper

# Charger les variables d'environnement
load_dotenv()


class FasopresseRealtimeMonitor:
    """Moniteur temps r√©el pour Fasopresse avec export JSON"""
    
    def __init__(self, check_interval: int = 600):
        """
        Initialise le moniteur
        
        Args:
            check_interval: Intervalle de v√©rification en secondes (600s = 10min)
        """
        self.check_interval = check_interval
        self.json_filename = 'fasopresse_realtime.json'
        self.posts_dict = {}  # Dict pour acc√®s rapide par post_id
        self.page_keywords = ['fasopresse', 'Fasopresse', 'p/Fasopresse']
        self.scraper = None
        self.is_logged_in = False
        
    def load_existing_data(self):
        """Charge les donn√©es existantes depuis le JSON"""
        if not os.path.exists(self.json_filename):
            print(f"üìÑ Nouveau fichier JSON √† cr√©er: {self.json_filename}")
            return
        
        try:
            with open(self.json_filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                posts = data.get('posts', [])
                for post in posts:
                    self.posts_dict[post['post_id']] = post
            print(f"üìÇ {len(self.posts_dict)} posts charg√©s depuis JSON")
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur chargement JSON: {e}")
    
    def save_to_json(self):
        """Sauvegarde toutes les donn√©es dans le JSON"""
        try:
            # Trier par date (plus r√©cent d'abord)
            sorted_posts = sorted(self.posts_dict.values(), 
                                key=lambda x: x['date_post'], 
                                reverse=True)
            
            # Calculer les statistiques
            total_engagement = sum(p['engagement_total'] for p in sorted_posts)
            total_likes = sum(p['likes'] for p in sorted_posts)
            total_comments = sum(p['comments'] for p in sorted_posts)
            total_shares = sum(p['shares'] for p in sorted_posts)
            
            output = {
                'posts': sorted_posts,
                'metadata': {
                    'total_posts': len(sorted_posts),
                    'last_update': datetime.now().isoformat(),
                    'total_engagement': total_engagement,
                    'total_likes': total_likes,
                    'total_comments': total_comments,
                    'total_shares': total_shares,
                    'page': 'Fasopresse',
                    'page_url': 'https://web.facebook.com/p/Fasopresse-Lactualit%C3%A9-du-Burkina-Faso-100067981629793/'
                }
            }
            
            with open(self.json_filename, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ {len(self.posts_dict)} posts sauvegard√©s dans {self.json_filename}")
        except Exception as e:
            print(f"‚ùå Erreur sauvegarde JSON: {e}")
    
    def check_and_update(self, email: str, password: str, page_url: str):
        """
        V√©rifie le dernier post et met √† jour les m√©triques de tous les posts
        """
        print(f"\n{'='*70}")
        print(f"üîç V√âRIFICATION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}")
        
        # Initialiser le scraper si n√©cessaire
        if not self.scraper:
            self.scraper = FacebookPlaywrightScraper(headless=True, page_keywords=self.page_keywords)
        
        # Scraper la page (r√©cup√®re 50 posts max)
        if not self.is_logged_in:
            print("üîê Connexion initiale...")
        else:
            print("‚ôªÔ∏è R√©utilisation de la session existante...")
        
        posts = self.scraper.scrape_page(
            page_url=page_url,
            email=email,
            password=password,
            max_posts=50
        )
        
        if posts:
            self.is_logged_in = True
        
        if not posts:
            print("‚ö†Ô∏è Aucun post r√©cup√©r√© lors de cette v√©rification")
            return
        
        # Mise √† jour des posts
        new_posts = 0
        updated_posts = 0
        
        for post in posts:
            post_id = post['post_id']
            
            if post_id not in self.posts_dict:
                # Nouveau post
                self.posts_dict[post_id] = post
                new_posts += 1
                print(f"üÜï Nouveau post d√©tect√©: {post_id[:30]}...")
            else:
                # Mise √† jour des m√©triques
                old_post = self.posts_dict[post_id]
                if (post['likes'] != old_post['likes'] or 
                    post['comments'] != old_post['comments'] or 
                    post['shares'] != old_post['shares']):
                    self.posts_dict[post_id] = post
                    updated_posts += 1
        
        # Sauvegarder les changements
        if new_posts > 0 or updated_posts > 0:
            self.save_to_json()
            print(f"‚úÖ {new_posts} nouveaux posts, {updated_posts} posts mis √† jour")
        else:
            print("‚ÑπÔ∏è Aucun changement d√©tect√©")
    
    def run_continuous(self, email: str, password: str, page_url: str):
        """
        Lance le monitoring en continu
        """
        print("\n" + "="*70)
        print("üöÄ D√âMARRAGE DU MONITORING FASOPRESSE")
        print("="*70)
        print(f"üìä Fichier de donn√©es: {self.json_filename}")
        print(f"‚è±Ô∏è Intervalle de v√©rification: {self.check_interval}s ({self.check_interval//60}min)")
        print(f"üåê Page: {page_url}")
        print(f"üîë Mots-cl√©s de validation: {', '.join(self.page_keywords)}")
        print("="*70)
        
        # Charger les donn√©es existantes
        self.load_existing_data()
        
        try:
            iteration = 0
            while True:
                iteration += 1
                print(f"\nüîÑ IT√âRATION #{iteration}")
                
                # V√©rifier et mettre √† jour
                try:
                    self.check_and_update(email, password, page_url)
                except Exception as e:
                    print(f"‚ùå Erreur lors de la v√©rification: {e}")
                    # R√©initialiser le scraper en cas d'erreur
                    if self.scraper:
                        self.scraper.close()
                        self.scraper = None
                        self.is_logged_in = False
                
                # Attendre avant la prochaine v√©rification
                print(f"\n‚è≥ Prochaine v√©rification dans {self.check_interval}s...")
                time.sleep(self.check_interval)
                
        except KeyboardInterrupt:
            print("\n\n‚õî Arr√™t du monitoring demand√© par l'utilisateur")
            if self.scraper:
                self.scraper.close()
            print("üëã Au revoir !")


def main():
    """Point d'entr√©e principal"""
    # R√©cup√©rer les credentials depuis les variables d'environnement
    email = os.getenv('FACEBOOK_EMAIL')
    password = os.getenv('FACEBOOK_PASSWORD')
    
    if not email or not password:
        print("‚ùå ERREUR: Variables d'environnement manquantes!")
        print("Cr√©ez un fichier .env avec:")
        print("FACEBOOK_EMAIL=votre_email")
        print("FACEBOOK_PASSWORD=votre_mot_de_passe")
        return
    
    # URL de la page Fasopresse
    page_url = "https://web.facebook.com/p/Fasopresse-Lactualit%C3%A9-du-Burkina-Faso-100067981629793/"
    
    # Cr√©er et lancer le moniteur
    monitor = FasopresseRealtimeMonitor(check_interval=600)  # 10 minutes
    monitor.run_continuous(email, password, page_url)


if __name__ == "__main__":
    main()
