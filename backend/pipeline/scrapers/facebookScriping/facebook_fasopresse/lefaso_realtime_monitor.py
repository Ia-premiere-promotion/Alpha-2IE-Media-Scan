"""
Script de monitoring temps rÃ©el de la page Facebook Lefaso.net
DÃ©tecte le dernier post et met Ã  jour les mÃ©triques dans un JSON (pas de doublons)
"""

import json
import time
import os
from datetime import datetime
from dotenv import load_dotenv
from facebook_playwright_scraper import FacebookPlaywrightScraper

# Charger les variables d'environnement
load_dotenv()


class LefasoRealtimeMonitor:
    """Moniteur temps rÃ©el pour Lefaso.net avec export JSON"""
    
    def __init__(self, check_interval: int = 600):
        """
        Initialise le moniteur
        
        Args:
            check_interval: Intervalle de vÃ©rification en secondes (600s = 10min)
        """
        self.check_interval = check_interval
        self.json_filename = 'lefaso_realtime.json'
        self.posts_dict = {}  # Dict pour accÃ¨s rapide par post_id
        self.page_keywords = ['lefaso.net', 'lefaso', 'Le Faso']
        self.scraper = None
        self.is_logged_in = False
        
    def load_existing_data(self):
        """Charge les donnÃ©es existantes depuis le JSON"""
        if not os.path.exists(self.json_filename):
            print(f"ğŸ“„ Nouveau fichier JSON Ã  crÃ©er: {self.json_filename}")
            return
        
        try:
            with open(self.json_filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                posts = data.get('posts', [])
                for post in posts:
                    self.posts_dict[post['post_id']] = post
            print(f"ğŸ“‚ {len(self.posts_dict)} posts chargÃ©s depuis JSON")
        except Exception as e:
            print(f"âš ï¸ Erreur chargement JSON: {e}")
    
    def save_to_json(self):
        """Sauvegarde toutes les donnÃ©es dans le JSON"""
        try:
            # Trier par date (plus rÃ©cent d'abord)
            sorted_posts = sorted(self.posts_dict.values(), 
                                key=lambda x: x['date_post'], 
                                reverse=True)
            
            # Calculer les statistiques
            total_engagement = sum(p['engagement_total'] for p in sorted_posts)
            total_likes = sum(p['likes'] for p in sorted_posts)
            total_comments = sum(p['comments'] for p in sorted_posts)
            total_shares = sum(p['shares'] for p in sorted_posts)
            
            output = {
                'posts': sorted_posts,
                'metadata': {
                    'total_posts': len(sorted_posts),
                    'last_update': datetime.now().isoformat(),
                    'total_engagement': total_engagement,
                    'total_likes': total_likes,
                    'total_comments': total_comments,
                    'total_shares': total_shares,
                    'page': 'Lefaso.net',
                    'page_url': 'https://web.facebook.com/lefaso.net'
                }
            }
            
            with open(self.json_filename, 'w', encoding='utf-8') as f:
                json.dump(output, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ {len(self.posts_dict)} posts sauvegardÃ©s dans {self.json_filename}")
        except Exception as e:
            print(f"âŒ Erreur sauvegarde JSON: {e}")
    
    def check_and_update(self, email: str, password: str, page_url: str):
        """
        VÃ©rifie le dernier post et met Ã  jour les mÃ©triques de tous les posts
        """
        print(f"\n{'='*70}")
        print(f"ğŸ” VÃ‰RIFICATION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}")
        
        # Initialiser le scraper si nÃ©cessaire
        if not self.scraper:
            self.scraper = FacebookPlaywrightScraper(headless=True, page_keywords=self.page_keywords)
        
        # Scraper la page (rÃ©cupÃ¨re 50 posts max)
        if not self.is_logged_in:
            print("ğŸ” Connexion initiale...")
        else:
            print("â™»ï¸ RÃ©utilisation de la session...")
        
        self.scraper.scrape_page(page_url, email, password, max_posts=50)
        self.is_logged_in = True
        
        if not self.scraper.posts_data:
            print("âš ï¸ Aucun post rÃ©cupÃ©rÃ©")
            return
        
        # DÃ©tecter le dernier post (le plus rÃ©cent)
        latest_post = self.scraper.posts_data[0]  # Le premier est le plus rÃ©cent
        
        # Statistiques
        new_posts = 0
        updated_posts = 0
        
        # Traiter tous les posts scrapÃ©s
        for post in self.scraper.posts_data:
            post_id = post['post_id']
            
            if post_id not in self.posts_dict:
                # Nouveau post dÃ©tectÃ©
                self.posts_dict[post_id] = {
                    'post_id': post_id,
                    'url': post['url'],
                    'date_post': post['date_post'],
                    'contenu': post['contenu'],
                    'likes': post['likes'],
                    'comments': post['comments'],
                    'shares': post['shares'],
                    'engagement_total': post['engagement_total'],
                    'commentaires': post.get('commentaires', []),
                    'last_update': datetime.now().isoformat()
                }
                new_posts += 1
                
                print(f"\nğŸ†• NOUVEAU POST dÃ©tectÃ©:")
                print(f"   ğŸ“ {post['contenu'][:100]}...")
                print(f"   ğŸ‘ {post['likes']} likes | ğŸ’¬ {post['comments']} comments | ğŸ”„ {post['shares']} shares")
            
            else:
                # Post existant - vÃ©rifier si les mÃ©triques ont changÃ©
                old_post = self.posts_dict[post_id]
                
                if (old_post['likes'] != post['likes'] or 
                    old_post['comments'] != post['comments'] or 
                    old_post['shares'] != post['shares']):
                    
                    # Mise Ã  jour
                    old_engagement = old_post['engagement_total']
                    new_engagement = post['engagement_total']
                    diff = new_engagement - old_engagement
                    
                    # Calculer les diffÃ©rences
                    diff_likes = post['likes'] - old_post['likes']
                    diff_comments = post['comments'] - old_post['comments']
                    diff_shares = post['shares'] - old_post['shares']
                    
                    self.posts_dict[post_id].update({
                        'likes': post['likes'],
                        'comments': post['comments'],
                        'shares': post['shares'],
                        'engagement_total': post['engagement_total'],
                        'commentaires': post.get('commentaires', []),
                        'last_update': datetime.now().isoformat()
                    })
                    
                    updated_posts += 1
                    
                    print(f"\nğŸ”„ MISE Ã€ JOUR:")
                    print(f"   ğŸ“ {post['contenu'][:70]}...")
                    print(f"   ğŸ“Š +{diff} engagement")
                    print(f"      ğŸ‘ {old_post['likes']} â†’ {post['likes']} (+{diff_likes})")
                    print(f"      ğŸ’¬ {old_post['comments']} â†’ {post['comments']} (+{diff_comments})")
                    print(f"      ğŸ”„ {old_post['shares']} â†’ {post['shares']} (+{diff_shares})")
        
        # RÃ©sumÃ©
        print(f"\n{'='*70}")
        print(f"ğŸ“Š RÃ‰SUMÃ‰:")
        print(f"   ğŸ†• {new_posts} nouveau(x) post(s)")
        print(f"   ğŸ”„ {updated_posts} mise(s) Ã  jour")
        print(f"   ğŸ’¾ {len(self.posts_dict)} posts au total dans JSON")
        print(f"{'='*70}")
        
        # Sauvegarder
        self.save_to_json()
    
    def start_monitoring(self, email: str, password: str, page_url: str):
        """
        DÃ©marre la boucle de monitoring
        """
        print("=" * 70)
        print("ğŸš€ MONITORING TEMPS RÃ‰EL - LEFASO.NET")
        print("=" * 70)
        print(f"ğŸ“ Page: {page_url}")
        print(f"â±ï¸ Intervalle: {self.check_interval}s ({self.check_interval // 60} min)")
        print(f"ğŸ’¾ Fichier JSON: {self.json_filename}")
        print("=" * 70)
        
        # Charger les donnÃ©es existantes
        self.load_existing_data()
        
        try:
            while True:
                # VÃ©rification
                self.check_and_update(email, password, page_url)
                
                # Attente avant prochaine vÃ©rification
                print(f"\nâ¸ï¸ Prochaine vÃ©rification dans {self.check_interval // 60} minutes...")
                print(f"   (Appuyez sur Ctrl+C pour arrÃªter)")
                time.sleep(self.check_interval)
        
        except KeyboardInterrupt:
            print("\n\nâ¹ï¸ Monitoring arrÃªtÃ© par l'utilisateur")
            print("ğŸ’¾ DerniÃ¨re sauvegarde en cours...")
            self.save_to_json()
            
            # Fermer le scraper
            if self.scraper and self.scraper.browser:
                self.scraper.close()
            
            print("âœ… TerminÃ© proprement")
        
        except Exception as e:
            print(f"\nâŒ Erreur inattendue: {e}")
            # Sauvegarder avant de quitter
            self.save_to_json()
            # Fermer le scraper
            if self.scraper and self.scraper.browser:
                self.scraper.close()


def main():
    """Point d'entrÃ©e principal"""
    
    # RÃ©cupÃ©rer les identifiants depuis .env
    email = os.getenv('FB_EMAIL')
    password = os.getenv('FB_PASSWORD')
    page_url = 'https://web.facebook.com/lefaso.net'
    
    if not email or not password:
        print("âŒ Erreur: Identifiants manquants")
        print("   CrÃ©ez un fichier .env avec:")
        print("   FB_EMAIL=votre_email@example.com")
        print("   FB_PASSWORD=votre_mot_de_passe")
        return
    
    # CrÃ©er et dÃ©marrer le moniteur
    monitor = LefasoRealtimeMonitor(check_interval=600)  # 10 minutes
    monitor.start_monitoring(email, password, page_url)


if __name__ == "__main__":
    main()
