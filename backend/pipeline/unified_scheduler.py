"""
Scheduler unifi√© pour lancer automatiquement les 2 pipelines:
- Pipeline WEB (m√©dias burkinab√®)
- Pipeline Facebook
Toutes les 10 minutes avec syst√®me de notifications
"""

import logging
import sys
from pathlib import Path
from datetime import datetime
import json
import threading
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.events import EVENT_JOB_EXECUTED, EVENT_JOB_ERROR

# Ajouter les paths n√©cessaires
sys.path.insert(0, str(Path(__file__).parent))
sys.path.insert(0, str(Path(__file__).parent.parent))

from pipeline.orchestrator import PipelineOrchestrator
from pipeline.scrapers.facebookScriping.facebook_orchestrator import FacebookOrchestrator

# Configuration du logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Fichier pour persister l'√©tat
STATE_FILE = Path(__file__).parent.parent / 'pipeline_state.json'

def load_pipeline_state():
    """Charge l'√©tat du pipeline depuis le fichier"""
    if STATE_FILE.exists():
        try:
            with open(STATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Erreur chargement √©tat: {e}")
    return {
        'is_running': False,
        'last_run': None,
        'last_result': None,
        'current_progress': None,
        'notifications': []
    }

def save_pipeline_state(state):
    """Sauvegarde l'√©tat du pipeline dans le fichier"""
    try:
        with open(STATE_FILE, 'w', encoding='utf-8') as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"‚ùå Erreur sauvegarde √©tat: {e}")

def add_notification(notification):
    """Ajoute une notification √† la liste (max 20)"""
    state = load_pipeline_state()
    
    # Ajouter l'ID et le statut read
    notification['id'] = datetime.now().timestamp()
    notification['read'] = False
    
    state['notifications'].insert(0, notification)
    # Garder seulement les 20 derni√®res
    state['notifications'] = state['notifications'][:20]
    save_pipeline_state(state)
    logger.info(f"üì¢ Notification ajout√©e: {notification['title']}")


def run_alerts_check():
    """
    üö® V√©rifie et g√©n√®re les alertes pour tous les m√©dias
    Appel√© toutes les heures
    """
    try:
        logger.info("üö® V√©rification des alertes...")
        
        # Importer les d√©pendances
        from supabase import create_client
        import os
        from dotenv import load_dotenv
        
        load_dotenv()
        
        # Cr√©er le client Supabase
        supabase_url = os.getenv('SUPABASE_URL')
        supabase_key = os.getenv('SUPABASE_SERVICE_ROLE_KEY')
        supabase = create_client(supabase_url, supabase_key)
        
        # Importer le g√©n√©rateur d'alertes
        sys.path.insert(0, str(Path(__file__).parent.parent))
        from utils.alert_generator import AlertGenerator
        
        generator = AlertGenerator(supabase)
        
        # R√©cup√©rer tous les m√©dias actifs
        medias = supabase.table('medias')\
            .select('id, name, followers, creation_date, is_active')\
            .eq('is_active', True)\
            .execute()
        
        total_alerts = 0
        
        for media in medias.data:
            # Calculer la r√©gularit√© (90 jours)
            from datetime import timedelta
            ninety_days_ago = datetime.utcnow() - timedelta(days=90)
            articles_90d = supabase.table('articles')\
                .select('date')\
                .eq('media_id', media['id'])\
                .gte('date', ninety_days_ago.isoformat())\
                .execute()
            
            dates_with_articles = set()
            for article in articles_90d.data:
                article_date = datetime.fromisoformat(article['date'].replace('Z', '+00:00')).date()
                dates_with_articles.add(article_date)
            
            days_with_publications = len(dates_with_articles)
            regularite = (days_with_publications / 90) * 100
            
            media['regularite'] = regularite
            
            # G√©n√©rer les alertes pour ce m√©dia
            alerts = generator.generate_alerts_for_media(media)
            
            # Sauvegarder les alertes
            for alert in alerts:
                if generator.save_alert(alert):
                    total_alerts += 1
                    
                    # Cr√©er une notification pour les alertes critiques et high
                    if alert['severite'] in ['critical', 'high']:
                        severity_emoji = 'üî¥' if alert['severite'] == 'critical' else 'üü†'
                        add_notification({
                            'type': 'alert',
                            'title': f'{severity_emoji} {alert["titre"]}',
                            'message': alert['message'],
                            'severity': alert['severite'],
                            'timestamp': datetime.now().isoformat()
                        })
        
        logger.info(f"‚úÖ V√©rification des alertes termin√©e: {total_alerts} nouvelles alertes")
        
        # Notification r√©capitulative si des alertes ont √©t√© cr√©√©es
        if total_alerts > 0:
            add_notification({
                'type': 'info',
                'title': f'üö® {total_alerts} nouvelles alertes d√©tect√©es',
                'message': f'V√©rification automatique des m√©triques termin√©e',
                'timestamp': datetime.now().isoformat()
            })
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification des alertes: {e}")
        import traceback
        traceback.print_exc()


def run_unified_pipeline():
    """
    Ex√©cute les 2 pipelines en parall√®le:
    1. Pipeline WEB (m√©dias burkinab√®)
    2. Pipeline Facebook
    """
    state = load_pipeline_state()
    
    # V√©rifier si un scraping est d√©j√† en cours (avec timeout de s√©curit√© de 30 min)
    if state.get('is_running', False):
        last_run = state.get('last_run')
        if last_run:
            from dateutil import parser
            last_run_time = parser.isoparse(last_run)
            elapsed = (datetime.now() - last_run_time).total_seconds()
            # Si le scraping est bloqu√© depuis plus de 30 minutes, on le d√©bloque
            if elapsed > 1800:  # 30 minutes
                logger.warning(f"‚ö†Ô∏è Scraping bloqu√© depuis {elapsed/60:.1f} min - D√©blocage forc√©")
                state['is_running'] = False
                save_pipeline_state(state)
            else:
                logger.warning(f"‚ö†Ô∏è Un scraping est d√©j√† en cours depuis {elapsed/60:.1f} min")
                return
        else:
            logger.warning("‚ö†Ô∏è Un scraping est d√©j√† en cours, passage ignor√©")
            return
    
    logger.info("=" * 80)
    logger.info("üöÄ LANCEMENT DES PIPELINES AUTOMATIQUES")
    logger.info("=" * 80)
    
    # Recharger l'√©tat pour pr√©server les notifications pr√©c√©dentes
    state = load_pipeline_state()
    state['is_running'] = True
    state['last_run'] = datetime.now().isoformat()
    state['current_progress'] = {
        'status': 'starting',
        'message': 'üöÄ D√©marrage des pipelines...',
        'timestamp': datetime.now().isoformat()
    }
    save_pipeline_state(state)
    
    # Notification de d√©marrage
    add_notification({
        'type': 'info',
        'title': 'Scraping automatique d√©marr√©',
        'message': 'Lancement des pipelines WEB et Facebook',
        'timestamp': datetime.now().isoformat()
    })
    
    web_stats = {}
    facebook_stats = {}
    
    # Fonction pour ex√©cuter le pipeline WEB dans un thread
    def run_web_pipeline():
        nonlocal web_stats
        try:
            # === PIPELINE 1: WEB ===
            logger.info("\nüì∞ === PIPELINE WEB - M√©dias burkinab√® ===")
            
            # Recharger l'√©tat pour pr√©server les notifications
            state = load_pipeline_state()
            state['current_progress'] = {
                'status': 'web_scraping',
                'message': 'üì∞ Scraping des sites web en cours...',
                'timestamp': datetime.now().isoformat()
            }
            save_pipeline_state(state)
            
            web_orchestrator = PipelineOrchestrator(include_facebook=False)
            web_stats = web_orchestrator.run_full_pipeline(
                max_articles_per_section=20,
                facebook_max_posts=0
            )
            logger.info(f"‚úÖ Pipeline WEB termin√©: {web_stats.get('total_inserted', 0)} articles ins√©r√©s")
            
            # Notification succ√®s WEB
            add_notification({
                'type': 'success',
                'title': 'Pipeline WEB termin√©',
                'message': f"{web_stats.get('total_inserted', 0)} nouveaux articles ins√©r√©s",
                'stats': web_stats,
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Pipeline WEB: {e}")
            add_notification({
                'type': 'error',
                'title': 'Erreur Pipeline WEB',
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            })
            web_stats = {'error': str(e), 'total_inserted': 0}
    
    # Fonction pour ex√©cuter le pipeline Facebook dans un thread
    def run_facebook_pipeline():
        nonlocal facebook_stats
        try:
            # === PIPELINE 2: FACEBOOK ===
            logger.info("\nüë• === PIPELINE FACEBOOK ===")
            
            # Recharger l'√©tat pour pr√©server les notifications
            state = load_pipeline_state()
            state['current_progress'] = {
                'status': 'facebook_scraping',
                'message': 'üë• Traitement des posts Facebook...',
                'timestamp': datetime.now().isoformat()
            }
            save_pipeline_state(state)
            
            fb_orchestrator = FacebookOrchestrator()
            facebook_stats = fb_orchestrator.run_full_pipeline()
            logger.info(f"‚úÖ Pipeline Facebook termin√©: {facebook_stats.get('inserted', 0)} posts ins√©r√©s")
            
            # Notification succ√®s Facebook
            add_notification({
                'type': 'success',
                'title': 'Pipeline Facebook termin√©',
                'message': f"{facebook_stats.get('inserted', 0)} nouveaux posts ins√©r√©s",
                'stats': {
                    'total_scraped': facebook_stats.get('total_posts', 0),
                    'total_inserted': facebook_stats.get('inserted', 0),
                    'total_skipped': facebook_stats.get('duplicates', 0) + facebook_stats.get('rejected', 0)
                },
                'timestamp': datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"‚ùå Erreur Pipeline Facebook: {e}")
            add_notification({
                'type': 'error',
                'title': 'Erreur Pipeline Facebook',
                'message': str(e),
                'timestamp': datetime.now().isoformat()
            })
            facebook_stats = {'error': str(e), 'inserted': 0}
    
    try:
        # üöÄ LANCER LES 2 PIPELINES EN PARALL√àLE
        logger.info("üöÄ Lancement des 2 pipelines EN PARALL√àLE...")
        
        web_thread = threading.Thread(target=run_web_pipeline, name="WebPipeline")
        facebook_thread = threading.Thread(target=run_facebook_pipeline, name="FacebookPipeline")
        
        # D√©marrer les 2 threads simultan√©ment
        web_thread.start()
        facebook_thread.start()
        
        # Attendre que les 2 threads se terminent
        web_thread.join()
        facebook_thread.join()
        
        logger.info("‚úÖ Les 2 pipelines sont termin√©s")
        
        # === R√âSUM√â FINAL ===
        total_inserted = web_stats.get('total_inserted', 0) + facebook_stats.get('inserted', 0)
        
        logger.info("\n" + "=" * 80)
        logger.info("üìä R√âSUM√â DES PIPELINES")
        logger.info("=" * 80)
        logger.info(f"üì∞ WEB: {web_stats.get('total_inserted', 0)} articles")
        logger.info(f"üë• FACEBOOK: {facebook_stats.get('inserted', 0)} posts")
        logger.info(f"‚úÖ TOTAL: {total_inserted} nouveaux contenus")
        logger.info("=" * 80)
        
        # Recharger l'√©tat pour pr√©server les notifications
        state = load_pipeline_state()
        state['is_running'] = False
        state['last_run'] = datetime.now().isoformat()
        state['last_result'] = {
            'success': True,
            'web_stats': web_stats,
            'facebook_stats': facebook_stats,
            'total_inserted': total_inserted,
            'timestamp': datetime.now().isoformat()
        }
        state['current_progress'] = None
        save_pipeline_state(state)
        
        # Notification finale de r√©sum√©
        add_notification({
            'type': 'success',
            'title': 'Pipelines termin√©s',
            'message': f"Total: {total_inserted} nouveaux contenus ins√©r√©s (WEB: {web_stats.get('total_inserted', 0)}, Facebook: {facebook_stats.get('inserted', 0)})",
            'stats': {
                'total_scraped': web_stats.get('total_scraped', 0) + facebook_stats.get('total_posts', 0),
                'total_inserted': total_inserted,
                'total_skipped': web_stats.get('total_skipped', 0) + facebook_stats.get('duplicates', 0) + facebook_stats.get('rejected', 0)
            },
            'timestamp': datetime.now().isoformat()
        })
        
    except Exception as e:
        logger.error(f"‚ùå ERREUR CRITIQUE: {e}")
        import traceback
        traceback.print_exc()
        
        # Recharger l'√©tat pour pr√©server les notifications
        state = load_pipeline_state()
        state['is_running'] = False
        state['last_run'] = datetime.now().isoformat()
        state['last_result'] = {
            'success': False,
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }
        state['current_progress'] = None
        save_pipeline_state(state)
        
        add_notification({
            'type': 'error',
            'title': 'Erreur critique',
            'message': f'Erreur lors de l\'ex√©cution des pipelines: {str(e)}',
            'timestamp': datetime.now().isoformat()
        })


def start_unified_scheduler():
    """
    D√©marre le scheduler unifi√© qui lance:
    - Les 2 pipelines (WEB + Facebook) toutes les 10 minutes
    - La v√©rification des alertes toutes les heures
    """
    scheduler = BackgroundScheduler()
    
    # Job 1: Pipelines toutes les 10 minutes
    scheduler.add_job(
        func=run_unified_pipeline,
        trigger='interval',
        minutes=10,
        id='unified_pipeline_job',
        name='Pipeline Unifi√© (WEB + Facebook)',
        replace_existing=True,
        max_instances=1  # Un seul job √† la fois
    )
    
    # Job 2: Alertes toutes les heures
    scheduler.add_job(
        func=run_alerts_check,
        trigger='interval',
        hours=1,
        id='alerts_check_job',
        name='V√©rification des alertes',
        replace_existing=True,
        max_instances=1
    )
    
    # Ex√©cuter la v√©rification des alertes au d√©marrage
    logger.info("üö® Lancement initial de la v√©rification des alertes...")
    try:
        run_alerts_check()
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la v√©rification initiale des alertes: {e}")
    
    # Listener pour les √©v√©nements
    def job_listener(event):
        if event.exception:
            logger.error(f"‚ùå Job failed: {event.exception}")
        else:
            logger.info(f"‚úÖ Job executed successfully")
    
    scheduler.add_listener(job_listener, EVENT_JOB_EXECUTED | EVENT_JOB_ERROR)
    
    scheduler.start()
    logger.info("‚úÖ Scheduler unifi√© d√©marr√©")
    logger.info("   üì∞ Pipeline WEB + üë• Pipeline Facebook toutes les 10 minutes")
    logger.info("   üö® V√©rification des alertes toutes les heures")
    logger.info("   ‚è∞ Prochaine ex√©cution des pipelines dans 10 minutes")
    
    return scheduler


if __name__ == '__main__':
    """
    Mode autonome: lance le scheduler et attend ind√©finiment
    """
    logger.info("üöÄ D√©marrage du scheduler unifi√© en mode autonome")
    
    scheduler = start_unified_scheduler()
    
    try:
        # Garder le script actif
        import time
        while True:
            time.sleep(60)
    except (KeyboardInterrupt, SystemExit):
        logger.info("üõë Arr√™t du scheduler")
        scheduler.shutdown()
